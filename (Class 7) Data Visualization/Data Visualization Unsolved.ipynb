{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a seemingly infinite number of different tools for data visualization in Python. For today, we're going to focus on Matplotlib and Seaborn. \n",
    "\n",
    "> Matplotlib is a standard, Python, 2D plotting library (https://matplotlib.org/) <br> \n",
    "> Seaborn is also a Python, data visualization library built atop Matplotlib (https://seaborn.pydata.org/)\n",
    "\n",
    "We'll also delve into some work with geographic plotting using geopandas [bokeh](https://bokeh.pydata.org/en/latest/index.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rendering our plots inline (aka, in our Jupyter notebook) and changing the layout a bit\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing all of our libraries\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting some more styling\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize': (20, 20)})\n",
    "matplotlib.style.use(['seaborn-talk', 'seaborn-ticks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We'll begin using our [NYPD Crashes csv.](https://data.cityofnewyork.us/Public-Safety/NYPD-Motor-Vehicle-Collisions-Crashes/h9gi-nx95) – each row in the csv represents a crash event with associated details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DTYPES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dtypes\n",
    "\n",
    "As usual, we need to take a moment and convert some of our dtypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATETIME COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Creation\n",
    "\n",
    "We also want to create two new columns, one called 'Injury' that hosts a true value if there was at least one injury in an accident, and another column called 'Death' that hosts a true value if there was at least one death in an accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE CREATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overplotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCATTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT MASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT MASK WITH NEW FIGSIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addressing Overplotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `sampling` \n",
    "\n",
    "We can specify how many points we want to plot by either passing an integer or fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE INT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE FRAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `marker size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARKER SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `marker transparency`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARKER TRANSPARENCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms, Density Plots, and Contour Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hexbin (Hexagonal Bin Plot) creates a 2-d histogram, where the color signals the number of points within a particular area; The gridsize parameter chooses the size of each bin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HEXBIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20, 15))\n",
    "\n",
    "sample = cleandf.sample(10000) # take sample because density plots take a while to computer\n",
    "\n",
    "sns.kdeplot(\n",
    "    sample.LONGITUDE,\n",
    "    sample.LATITUDE,\n",
    "    gridsize=100,  # controls the resolution\n",
    "    cmap=plt.cm.rainbow,  # color scheme\n",
    "    shade=  # whether to have a density plot (True), or just the contours (False)\n",
    "    True,\n",
    "    alpha=0.5,\n",
    "    shade_lowest=False,\n",
    "    n_levels=50  # how many contours/levels to have\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contour Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20, 15))\n",
    "\n",
    "sample = cleandf.sample(10000)\n",
    "\n",
    "sns.kdeplot(\n",
    "    sample.LONGITUDE,\n",
    "    sample.LATITUDE,\n",
    "    gridsize=100,\n",
    "    cmap=plt.cm.rainbow,\n",
    "    shade=False,\n",
    "    shade_lowest=False,\n",
    "    n_levels=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining plots\n",
    "\n",
    "We can combine multiple plots using the ax parameter (think of 'ax' as representative of an individual plot). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Geographic Boundaries using Bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAT LONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LISTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.bokeh.org/en/latest/\n",
    "\n",
    "from bokeh.io import output_file, show \n",
    "from bokeh.models import *\n",
    "\n",
    "\n",
    "map_options = GMapOptions(lat=40.7128, lng=-74.0060, map_type=\"roadmap\", zoom=11)\n",
    "\n",
    "plot = GMapPlot(x_range=Range1d(), y_range=Range1d(), map_options=map_options,api_key = \"{KEY HERE}\")\n",
    "\n",
    "source = ColumnDataSource(\n",
    "    data = dict(\n",
    "        lat=lat_list,\n",
    "        lon=lon_list,\n",
    "        date = date_list,\n",
    "        time = time_list,\n",
    "        borough = borough_list, \n",
    "        vehicle = vehicle_list\n",
    "    ))\n",
    "\n",
    "circle = Circle(x=\"lon\", y=\"lat\", size=15, fill_color=\"blue\", fill_alpha=0.8, line_color=None)\n",
    "plot.add_glyph(source, circle)\n",
    "\n",
    "plot.add_tools(PanTool(), WheelZoomTool(), BoxSelectTool(), BoxZoomTool())\n",
    "\n",
    "plot.title.text=\"NYC Accidents\"\n",
    "\n",
    "plot.add_tools(HoverTool(\n",
    "    tooltips=[\n",
    "        ( 'date',   '@date' ),\n",
    "        ( 'time',  '@time' ), \n",
    "        ( 'borough', '@borough' ), \n",
    "        ( 'vehicle', '@vehicle' )\n",
    "    ],\n",
    "\n",
    "    formatters={\n",
    "        'date' : 'datetime', # use 'datetime' formatter for 'date' field\n",
    "        'time' : 'printf',\n",
    "        'borough' : 'numeral',\n",
    "        'vehicle' : 'numeral'\n",
    "    },\n",
    "\n",
    "    mode='vline'\n",
    "))\n",
    "\n",
    "# output_file(\"gmap_plot.html\")\n",
    "\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Analyzing Citibike Station Activity using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import pandas as pd\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use(['seaborn-talk', 'seaborn-ticks', 'seaborn-whitegrid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's fetch our data as we did in week 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike in Week 1, though, we are using a script that runs continuously using a crontab (seen below) so that our database is continually populating with recent data. \n",
    "\n",
    "The .py script is called citibike_cron_script.py and can be found in the Class 7 folder of the course repo. \n",
    "\n",
    "> The crontab used is: \n",
    ">> */1 * * * * /Users/siegmanA/anaconda3/bin/python $(which python3) ~/Desktop/NYU-Projects-in-Programming-Fall-2019/\\(Class\\ 7\\)\\ Data\\ Visualization/citibike_cron_script.py >> ~/Desktop/tmp/citiCron.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to create a query that gets us the average capacity of a given station in hourly intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"\"\"SELECT station_id,\n",
    "                    stationName,\n",
    "                    availableBikes, \n",
    "                    availableDocks,\n",
    "                    totalDocks,\n",
    "                    latitude, \n",
    "                    longitude,\n",
    "                    lastCommunicationTime\n",
    "                FROM StationsData\"\"\", con=con)\n",
    "\n",
    "df['lastCommunicationTime'] = pd.to_datetime(df['lastCommunicationTime'], format='%Y-%m-%d %H:%M:%S %p')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: \n",
    "\n",
    "Create a new column in our df called 'percent_full' that tells us how full a bike station is at a given time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Time Series per Station\n",
    "\n",
    "Let's create a pivot table to examine the time series for individual stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIMESERIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there's an erroneous entry where we have a last communication time from 1969. Let's get rid of that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP 1969"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIMESERIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we plot that over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's limit our plot to just two stations:\n",
    "* Station 3260 at \"Mercer St & Bleecker St\"\n",
    "* Station 161 at \"LaGuardia Pl & W 3 St\"\n",
    "\n",
    "which are nearby and tend to exhibit similar behavior. Remember that the list of stations is [available as a JSON](https://feeds.citibikenyc.com/stations/stations.json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAGUARDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIMESERIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2:\n",
    "\n",
    "Plot a timeseries graph for stations 3260 and 161 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Bike Stations with Similar Behavior\n",
    "\n",
    "For our next analysis, we are going to try to find bike stations that have similar behaviors over time. A very simple technique that we can use to find similar time series is to treat the time series as vectors, and compute their correlation. Pandas provides the `corr` function that can be used to calculate the correlation of columns. (If we want to compute the correlation of rows, we can just take the transpose of the dataframe using the `transpose()` function, and compute the correlations there.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEARSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the similarities of the two stations that we examined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMILARITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 393: E 5 St & Avenue C\n",
    "# 2003: 1 Ave & E 18 St\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For bookkeeping purposes, we are going to drop stations that generate NaN values, as we cannot use such entries for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of stations with non-NaN similarity per station\n",
    "\n",
    "check = similarities.count()\n",
    "\n",
    "# find the number of stations with less than the max number of similarities\n",
    "\n",
    "todrop = check[check < check.max()].index.values\n",
    "similarities.drop(todrop, axis='index', inplace=True)\n",
    "similarities.drop(todrop, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Based on Distances\n",
    "\n",
    "Without explaining too much about clustering, we are going to use a clustering technique and cluster together bike stations that are \"nearby\" according to our similarity analysis. For this, we need to first convert our similarities to distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to convert our **similarities** into **distance** metrics. Our distance values will be always positive, and bounded between 0 and 1.\n",
    "\n",
    "* If two stations have correlation 1, they behave identically, and therefore have distance 0, \n",
    "* If two stations have correlation -1, they have exactly the oppositite behaviors, and therefore we want to have distance 1 (the max) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity goes from -1 to 1, so 1-similarity goes from 0 to 2.\n",
    "# so, we multiply with 0.5 to get it between 0 and 1, and then take the square\n",
    "\n",
    "distances = ((.5*(1-similarities))**2)\n",
    "distances.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering code is very simple: The code below will create two groups of stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cluster = KMeans(n_clusters=2)\n",
    "cluster.fit(distances.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now take the results of the clustering and associate each of the data points into a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(list(zip(distances.index.values.tolist(), cluster.labels_)), columns = [\"station_id\", \"cluster\"])\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many stations in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.pivot_table(\n",
    "    index = 'cluster',\n",
    "    aggfunc = 'count'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Time Series Clusters\n",
    "\n",
    "We will start by assining a color to each cluster, so that we can plot each station-timeline with the cluster color. (We put a long list of colors, so that we can play with the number of clusters in the earlier code, and still get nicely colored results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colors = list(['red','black', 'green', 'magenta', 'yellow', 'blue', 'white', 'cyan'])\n",
    "labels['color'] = labels['cluster'].apply(lambda cluster_id : colors[cluster_id]) \n",
    "labels.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_plot = station_timeseries.plot(\n",
    "    alpha=0.5, \n",
    "    legend=False, \n",
    "    figsize=(20,5), \n",
    "    linewidth=1,\n",
    "    color=labels['color'],\n",
    "    xlim=('2019-10-10 06', '2019-10-10 06:30'),\n",
    "    ylim=(0,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot still looks messy. Let's try to plot instead a single line for each cluster. To represent the cluster, we are going to use the _median_ fullness value across all stations that belong to a cluster, for each timestamp. For that, we can again use a pivot table: we define the `communication_time` as one dimension of the table, and `cluster` as the other dimension, and we use the `median` function. \n",
    "\n",
    "For that, we first _join_ our original dataframe with the results of the clustering, using the `merge` command, and add an extra column that includes the clusterid for each station. Then we compute the pivot table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "median_cluster = df.merge(\n",
    "    labels,\n",
    "    how='inner',\n",
    "    on='station_id'\n",
    ").pivot_table(\n",
    "    index='lastCommunicationTime', \n",
    "    columns='cluster', \n",
    "    values='percent_full', \n",
    "    aggfunc='median'\n",
    ")\n",
    "\n",
    "median_cluster.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot the medians for the two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_cluster.plot(\n",
    "    figsize=(20,5), \n",
    "    linewidth = 2, \n",
    "    alpha = 0.75,\n",
    "    color=colors,\n",
    "    ylim = (0,1),\n",
    "    xlim=('2019-10-10 06', '2019-10-10 06:05'),\n",
    "    grid = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just for fun and for visual decoration, let's put the two plots together. We are going to fade a lot the individual station time series (by putting the `alpha=0.005`) and we are going to make more prominent the median lines by increasing their linewidths. We will limit our plot to one week's worth of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_plot = station_timeseries.plot(\n",
    "    alpha=0.005, \n",
    "    legend=False, \n",
    "    figsize=(20,5), \n",
    "    color=labels[\"color\"]\n",
    ")\n",
    "\n",
    "median_cluster.plot(\n",
    "    figsize=(20,5), \n",
    "    linewidth = 3, \n",
    "    alpha = 0.5,\n",
    "    color=colors, \n",
    "    xlim=('2019-10-10 06', '2019-10-10 06:05'),\n",
    "    ylim=(0,1),\n",
    "    ax = stations_plot\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
