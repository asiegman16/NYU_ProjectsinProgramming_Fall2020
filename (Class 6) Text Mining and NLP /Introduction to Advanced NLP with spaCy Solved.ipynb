{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Today we are going to learn about NLP (Natural Language Processing) using spaCy, an open source library for advanced NLP. \n",
    "\n",
    "For more on spaCy, you can check out their site: https://spacy.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Natural Language Processing (NLP) is a subfield of Artificial Intelligence that deals with the 'understanding' of language. \n",
    "\n",
    "For the purposes of this tutorial, we are going to look at the very basics of NLP, including using spaCy, an open-source NLP library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en import English # import the English language class\n",
    "\n",
    "nlp = English() # create an nlp object; this object contains the processing pipeline, which you \n",
    "                # ultimately use to analyze the text\n",
    "    \n",
    "doc = nlp(\"Hello world!\") # \"Hello world!\" becomes the text that we want to analyze\n",
    "                          # when you process a text with the nlp object, spaCy creates a Doc object\n",
    "    \n",
    "for token in doc: # for every token in our Doc object (a token being a word or character)...\n",
    "    print(token.text) # simply print out that token "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Indexing\n",
    "\n",
    "Similar to how you index through a list in Python, you can index through a Doc to retreive tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(doc) # print the full text \n",
    "print(doc[1].text) # print the second token (remember, Python is 0-index) in our Doc object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spans\n",
    "\n",
    "You can also use 'span' which lets you take a slice of the Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "span = doc[0:2] # this will give us the first and second (again, remember, Python is 0-index)\n",
    "                # the span is not inclusive, so we don't actually get the third token (second index)\n",
    "\n",
    "print(doc) # print the full text\n",
    "print(span.text) # print the second and third tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What else can we do with spaCy? \n",
    "\n",
    "Tokens have lots of attributes associated with them! For instance: \n",
    "\n",
    "1. is_alpha returns boolean indicating if a token consists of an alphanumeric value\n",
    "2. is_punct returns boolean indicating if a token is punctuation\n",
    "3. like_num returns boolean indicating if a token resembles a number \n",
    "\n",
    "These are all called \"lexical attributes\" – they refer to the entry in the vocabulary and don't depend on the token's context. (More on that later). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"The earnings report will be released at 5 pm sharp.\") # the text we want to work with \n",
    "\n",
    "print('Index: ', [token.i for token in doc]) # i being the index of the token in the Doc\n",
    "print('Text: ', [token.text for token in doc]) # return the text of the token\n",
    "\n",
    "print(\" \") # just so we have some nice spacing in our results below...\n",
    "\n",
    "print('is_alpha:', [token.is_alpha for token in doc]) # if token consists of an alphanumeric value\n",
    "print('is_punct:', [token.is_punct for token in doc]) # if token is punctuation\n",
    "print('like_num:', [token.like_num for token in doc]) # if token resembles a number (e.g., '10' or 'TEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise 1:\n",
    "\n",
    "Imagine you are charged with reporting on a long press release, and you just want to know where in the document a percent increase or percent decrease is mentioned..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"In 2012, earnings were hovering around 60%, verus in 2019 where they are less than 4% – a 93% decrease.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use next_token, .like_num, .text, and find any percentage value mentiond in the doc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc: # for every token in our Doc object...\n",
    "    if token.like_num: # if the token resembles a numerical value...\n",
    "        next_token = doc[token.i + 1] # look at the token following that numerical value...\n",
    "        if next_token.text == \"%\": # if that token is a \"%\" sign...\n",
    "            next_token = doc[next_token.i + 1] # look at the token following the \"%\"\n",
    "            if next_token.text == \"increase\" or next_token.text == \"decrease\": # if the token after the % is the word \"decrease\" or \"increase\"...\n",
    "                print(\"Percentage found:\", token.text, next_token.text ) # we know we have found a percentage value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pre-Built Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import spacy \n",
    "\n",
    "nlp = spacy.load('en') # loading in the package we just downloaded...\n",
    "\n",
    "doc = nlp(\"Adidas AG and Gap Inc. are among those at the end of the long supply chain that travel through \\\n",
    "           China’s northwest region of Xinjiang.\") # this is the text we want to analyze \n",
    "                                                   # that '\\' above just lets me split the text into a new \n",
    "                                                   # line in my notebook, and isn't part of the text itself\n",
    "\n",
    "for token in doc: # for each token in our Doc...\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text) # print the following:\n",
    "    \n",
    "    # .pos_ will give us the parts of speech for each token\n",
    "    # .dep_ will give us the predicted dependency label \n",
    "    # .head.text will give us the 'syntactic head token' (think of it as the parent token this word is attached to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `ent.label_`\n",
    "\n",
    "Can be used to decipher entities..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Adidas AG and Gap Inc. are among those at the end of the long supply chain that travel through \\\n",
    "           China’s northwest region of Xinjiang.\")\n",
    "\n",
    "for ent in doc.ents: # for each entity in our Doc...\n",
    "    print(ent.text, ent.label_) # print it alongside its label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `.explain`\n",
    "\n",
    "Can be used to get quick definitions of common tags and labels, you can use \".explain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"GPE = \" + spacy.explain('GPE'))\n",
    "print(\"ORG = \" + spacy.explain('ORG'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "spaCy also lets you write rules to find words and/or phrases in a text. Similar to Regular Expressions, but with some major benefits unique to spaCy. \n",
    "\n",
    "In particular, it allows you to match on Doc objects (not just strings), use the model's prediction capabilities, and match on tokens and token attributes. Match patterns in spaCy are comprised of lists of dictionaries, and each dictionary describes one token. \n",
    "\n",
    "The keys in the dictionary are the names of the token attributes, and are mapped to their expected value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"New iPhone X release date leaked as Apple reveals pre-orders by mistake.\") # our text\n",
    "\n",
    "from spacy.matcher import Matcher # import the matcher\n",
    "matcher = Matcher(nlp.vocab) # initialize the matcher\n",
    "\n",
    "pattern = [{'TEXT':'iPhone'}, {'TEXT':'X'}]    # match these exact token texts\n",
    "\n",
    "matcher.add('IPHONE_PATTERN', None, pattern) # add the pattern to the matcher\n",
    "\n",
    "matches = matcher(doc) # call the matcher on our Doc and store the result as a list called 'matches'\n",
    "\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You'll note that the matcher returns a list of tuples. Each tuple consists of three values: \n",
    "\n",
    "    1. The match ID\n",
    "    2. The start index of the matched span\n",
    "    3. The end index of the matched span\n",
    "    \n",
    "Fortunately, we can iterate over our matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for match_id, start, end in matches: \n",
    "    matched_span = doc[start:end] # start = start index of matched span; end = end index of matched span\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Remember, you can also match on lexical attributes and token attributions. For instance, below we are going to look for five tokens: \n",
    "\n",
    "1. A token consisting of only digits\n",
    "2. Two, case-insensitive tokens for the words \"revenue\" and \"up\"\n",
    "3. Another token that consists of only digits\n",
    "4. A punctuation token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Earnings are in today! 2019 Revenue up 45%! This is the highest revenue in 5 years.\")\n",
    "\n",
    "pattern = [\n",
    "    {'IS_DIGIT': True}, # looking for a token consisting of only digits\n",
    "    {'LOWER': 'revenue'}, # looking for the word \"revenue\"\n",
    "    {'LOWER': 'up'}, # looking for the word \"up\"\n",
    "    {'IS_DIGIT': True}, # looking for a token consisting of only digits\n",
    "    {'IS_PUNCT': True} # looking for a punctuation token \n",
    "]\n",
    "\n",
    "matcher.add('REVENUE_PATTERN', None, pattern) # add the pattern to the matcher\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches: \n",
    "    matched_span = doc[start:end] # start = start index of matched span; end = end index of matched span\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## A note on Operators and Quantifiers.\n",
    "\n",
    "Operators and Quantifiers let you define how often a token should be matched. \n",
    "\n",
    "An Operator can have one of four values: \n",
    "\n",
    "1. An \"!\" negates the token, so it's matched 0 times\n",
    "2. A \"?\" makes the token optional, so it matches 0 or 1 times\n",
    "3. A \"+\" matches a token 1 or more times\n",
    "4. A \"*\" matches a token 0 or more times\n",
    "\n",
    "Below, the \"?\" Operator makes the determiner token optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Earnings are in today! 2019 Revenue up 45% for Company X. This is the highest revenue in 5 years.\")\n",
    "\n",
    "# note that our text above has changed... \n",
    "\n",
    "pattern = [\n",
    "    {'IS_DIGIT': True}, # looking for a token consisting of only digits\n",
    "    {'LOWER': 'revenue'}, # looking for the word \"revenue\"\n",
    "    {'LOWER': 'up'}, # looking for the word \"up\"\n",
    "    {'IS_DIGIT': True}, # looking for a token consisting of only digits\n",
    "    {'IS_PUNCT': True, 'OP' : '?'} # looking for an OPTIONAL punctuation token\n",
    "]\n",
    "\n",
    "matcher.add('REVENUE_PATTERN', None, pattern) # add the pattern to the matcher\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luckily, Spacy also allows us to use Regular Expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "doc = nlp(\"Earnings are in today! 2019 Revenue up 45% for Company X. This is the highest revenue in 5 years.\")\n",
    "\n",
    "expression = r'[Rr]evenue (up|down)' \n",
    "\n",
    "for match in re.finditer(expression, doc.text):\n",
    "    start, end = match.span()\n",
    "    span = doc.char_span(start, end)\n",
    "    print(\"Found match:\", span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Using the text provided below, create a pattern match that finds any news about a possible merger or acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"It is rumored that Google bought Apple.\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'ORG':\n",
    "        print(ent.text)\n",
    "        next_token = doc[ent.start + 1]\n",
    "        if next_token.text in (\"bought\", \"sold\", \"acquired\"):\n",
    "            print(next_token.text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "spaCy stores all shared data in a vocabulary, which includes words, as well as the labeled schemas for tags and entities. It also uses a hash function to generate an ID for each string, which is stored in a string store and is available via nlp.vocab.strings\n",
    "\n",
    "This string store is ultimately a lookup table whereby you can look up a string to get its hash, or, look up a hash to get the string. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"I love coffee\") # our text\n",
    "\n",
    "print('hash value:', nlp.vocab.strings['coffee']) # print the hash value given the text\n",
    "print('string value:', nlp.vocab.strings[3197928453018144401]) # print the text value given the hash "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "spaCy even lets you compare two objects to predict how similar they are. \n",
    "\n",
    "These objects can be documents, spans, or single tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# !sudo python -m spacy download en_core_web_md\n",
    "\n",
    "# !python -m spacy download en_core_web_md\n",
    "\n",
    "import en_core_web_md # you only have to download it in the line above if you didn't earlier\n",
    "\n",
    "nlp = en_core_web_md.load()\n",
    "\n",
    "# compare two documents\n",
    "\n",
    "doc1 = nlp(\"I like fast food\") # doc 1 to be compared\n",
    "doc2 = nlp(\"I like pizza\") # doc 2 to be compared\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also compare two tokens: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# nlp = en_core_web_md.load()\n",
    "\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "\n",
    "# compare two tokens\n",
    "\n",
    "token1 = doc[2] # the word \"pizza\"\n",
    "token2 = doc[4] # the word \"pasta\"\n",
    "\n",
    "print(token1.similarity(token2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"TV and books\")\n",
    "\n",
    "token1, token2 = doc[0], doc[2]\n",
    "\n",
    "similarity = token1.similarity(token2) # get the similarity of the tokens \"TV\" and \"books\"\n",
    "\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or, a document with a token: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# nlp = en_core_web_md.load()\n",
    "\n",
    "# compare a document with a token\n",
    "\n",
    "doc = nlp(\"I like being clean\") # this full text\n",
    "token = nlp(\"I also like soap\")[3] # the word \"soap\"\n",
    "\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And, last but not least, a span with a document: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# nlp = en_core_web_md.load()\n",
    "\n",
    "# compare a span with a document\n",
    "\n",
    "span = nlp(\"I like pizza and pasta\")[2:5] # the words \"pizza and pasta\"\n",
    "doc = nlp(\"McDonalds sells burgers\") # this full text\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BeautifulSoup + spaCy\n",
    "\n",
    "Now, let's use some of the BeautifulSoup to analyze some text from an online source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r = requests.get('https://www.vice.com/en_us/article/a35ve5/what-it-would-take-for-the-next-president-to-cancel-all-student-debt') \n",
    "    # for more on the requests library check out this tutorial from RealPython: \n",
    "    # https://realpython.com/python-requests/\n",
    "            \n",
    "soup = BeautifulSoup(r.text,'html') # we are going to turn that URL into 'soup', aka, we are going to be \n",
    "                                    # able to see it's metadata For more on BeautifulSoup, check out: \n",
    "                                    # https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "            \n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We know that what we're interested in is the text of this article. So, let's see what that looks like in the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "paragraphs = [i.get_text() for i in soup.find_all('p')] # find all of the <p> elements in our text\n",
    "\n",
    "print(paragraphs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "article = '\\n'.join(paragraphs) # join all of those paragraphs together\n",
    "\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Exercise 3:\n",
    "\n",
    "What if we want to know what entities are mentioned in the article? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "doc = nlp(article) # our text is going to be the article text above (that first 500 characters)\n",
    "\n",
    "for ent in doc.ents: # for each entity in our Doc...\n",
    "    print(ent.text, ent.label_) # print that entity aside its label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise 4:\n",
    "\n",
    "What if we want to know the similarity between the first and last sentences of the aritcle? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc1 = nlp(paragraphs[3])\n",
    "doc2 = nlp(paragraphs[-3])\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An enormous thank you to spaCy, whose existing online courses (https://course.spacy.io/chapter1) were the basis for this Jupyter-ized tutorial. For more information on spaCy's existing online course, check out https://github.com/ines/spacy-course#-faq"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
